{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset fineweb-edu (/scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11)\n",
      "INFO:datasets.builder:Generating dataset fineweb-edu (/scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11)\n",
      "Downloading and preparing dataset fineweb-edu/sample-10BT to /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11...\n",
      "INFO:datasets.builder:Downloading and preparing dataset fineweb-edu/sample-10BT to /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11...\n",
      "Downloading took 2.0 min\n",
      "INFO:datasets.download.download_manager:Downloading took 2.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "INFO:datasets.builder:Generating train split\n",
      "Generating train split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9672101/9672101 [05:56<00:00, 27166.64 examples/s]\n",
      "All the splits matched successfully.\n",
      "INFO:datasets.utils.info_utils:All the splits matched successfully.\n",
      "Dataset fineweb-edu downloaded and prepared to /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11. Subsequent calls will reuse this data.\n",
      "INFO:datasets.builder:Dataset fineweb-edu downloaded and prepared to /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9672101/9672101 [06:53<00:00, 23393.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the storage path (change this to your preferred directory)\n",
    "storage_path = \"/scratch/users/hafezgh/fineweb-10B\"\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Enable logging to show download progress\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load and store the dataset in the specified folder\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", \n",
    "                  name=\"sample-10BT\", \n",
    "                  split=\"train\", \n",
    "                  streaming=False, \n",
    "                  cache_dir=storage_path)\n",
    "for _ in tqdm(fw, total=len(fw)):  # If len(fw) is available\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|███████████████| 2110/2110 [00:00<00:00, 3123.90it/s]\n",
      "Loading dataset shards: 100%|█████████████████| 98/98 [00:00<00:00, 2471.54it/s]\n",
      "Tokenizing Dataset:   0%|      | 1024/9672101 [00:02<6:12:51, 432.30 examples/s]^C\n",
      "Tokenizing Dataset:   0%|     | 1024/9672101 [00:04<12:24:05, 216.62 examples/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hafezgh/titans-pytorch/tokenize_dataset.py\", line 70, in <module>\n",
      "    processed_dataset = raw_dataset.map(\n",
      "  File \"/home/hafezgh/.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 562, in wrapper\n",
      "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
      "  File \"/home/hafezgh/.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3079, in map\n",
      "    for rank, done, content in Dataset._map_single(**dataset_kwargs):\n",
      "  File \"/home/hafezgh/.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3526, in _map_single\n",
      "    writer.write_batch(batch)\n",
      "  File \"/home/hafezgh/.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 605, in write_batch\n",
      "    arrays.append(pa.array(typed_sequence))\n",
      "  File \"pyarrow/array.pxi\", line 252, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 114, in pyarrow.lib._handle_arrow_array_protocol\n",
      "  File \"/home/hafezgh/.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 228, in __arrow_array__\n",
      "    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python /home/hafezgh/titans-pytorch/tokenize_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (memory-2025-02-py310)",
   "language": "python",
   "name": "memory-2025-02-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
