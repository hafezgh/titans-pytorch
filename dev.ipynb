{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.vmgr_repo/memory-2025-02-py310/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset fineweb-edu (/scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11)\n",
      "INFO:datasets.builder:Generating dataset fineweb-edu (/scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11)\n",
      "Downloading and preparing dataset fineweb-edu/sample-10BT to /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11...\n",
      "INFO:datasets.builder:Downloading and preparing dataset fineweb-edu/sample-10BT to /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11...\n",
      "Downloading took 2.0 min\n",
      "INFO:datasets.download.download_manager:Downloading took 2.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "INFO:datasets.builder:Generating train split\n",
      "Generating train split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9672101/9672101 [05:56<00:00, 27166.64 examples/s]\n",
      "All the splits matched successfully.\n",
      "INFO:datasets.utils.info_utils:All the splits matched successfully.\n",
      "Dataset fineweb-edu downloaded and prepared to /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11. Subsequent calls will reuse this data.\n",
      "INFO:datasets.builder:Dataset fineweb-edu downloaded and prepared to /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9672101/9672101 [06:53<00:00, 23393.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the storage path (change this to your preferred directory)\n",
    "storage_path = \"/scratch/users/hafezgh/fineweb-10B\"\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Enable logging to show download progress\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load and store the dataset in the specified folder\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", \n",
    "                  name=\"sample-10BT\", \n",
    "                  split=\"train\", \n",
    "                  streaming=False, \n",
    "                  cache_dir=storage_path)\n",
    "for _ in tqdm(fw, total=len(fw)):  # If len(fw) is available\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n",
      "INFO:datasets.builder:Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11\n",
      "INFO:datasets.info:Loading Dataset info from /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11\n",
      "Found cached dataset fineweb-edu (/scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11)\n",
      "INFO:datasets.builder:Found cached dataset fineweb-edu (/scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11)\n",
      "Loading Dataset info from /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11\n",
      "INFO:datasets.info:Loading Dataset info from /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11\n",
      "Tokenizing Dataset:   0%|                                                                                                                                                          | 0/9672101 [00:00<?, ? examples/s]Caching processed dataset at /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11/cache-333080b6e720f558.arrow\n",
      "INFO:datasets.arrow_dataset:Caching processed dataset at /scratch/users/hafezgh/fineweb-10B/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11/cache-333080b6e720f558.arrow\n",
      "Tokenizing Dataset:   0%|                                                                                                                                            | 6144/9672101 [00:14<6:26:46, 416.52 examples/s]"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # needed if eos_token != pad_token\n",
    "\n",
    "# Load dataset\n",
    "raw_dataset = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb-edu\",\n",
    "    name=\"sample-10BT\",\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    "    cache_dir=\"/scratch/users/hafezgh/fineweb-10B\"\n",
    ")\n",
    "\n",
    "def tokenize_and_chunk(examples):\n",
    "    \"\"\"\n",
    "    Splits each text into 4096-token chunks, then returns them as \n",
    "    lists of equal length for 'input_ids' and 'attention_mask'.\n",
    "    \"\"\"\n",
    "    chunk_size = 4096\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "\n",
    "    for text in examples[\"text\"]:\n",
    "        tokens = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"np\",\n",
    "            truncation=False,\n",
    "            padding=False\n",
    "        )\n",
    "        input_ids = tokens[\"input_ids\"][0]\n",
    "        attention_mask = tokens[\"attention_mask\"][0]\n",
    "\n",
    "        # Break into chunks\n",
    "        for i in range(0, len(input_ids), chunk_size):\n",
    "            chunk_ids = input_ids[i : i + chunk_size]\n",
    "            chunk_mask = attention_mask[i : i + chunk_size]\n",
    "\n",
    "            # Pad if needed\n",
    "            if len(chunk_ids) < chunk_size:\n",
    "                pad_len = chunk_size - len(chunk_ids)\n",
    "                chunk_ids = np.pad(\n",
    "                    chunk_ids,\n",
    "                    (0, pad_len),\n",
    "                    constant_values=tokenizer.eos_token_id\n",
    "                )\n",
    "                chunk_mask = np.pad(\n",
    "                    chunk_mask,\n",
    "                    (0, pad_len),\n",
    "                    constant_values=0\n",
    "                )\n",
    "\n",
    "            # Collect the chunk\n",
    "            all_input_ids.append(chunk_ids.tolist())\n",
    "            all_attention_masks.append(chunk_mask.tolist())\n",
    "\n",
    "    # Return a dict of lists (one column per list)\n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_masks\n",
    "    }\n",
    "\n",
    "# Remove ALL columns from the original dataset.\n",
    "# This ensures we don't get a length mismatch on the old columns.\n",
    "all_cols = raw_dataset.column_names\n",
    "\n",
    "processed_dataset = raw_dataset.map(\n",
    "    tokenize_and_chunk,\n",
    "    batched=True,\n",
    "    batch_size=1024,\n",
    "    remove_columns=all_cols,\n",
    "    desc=\"Tokenizing Dataset\"\n",
    ")\n",
    "\n",
    "# Save final dataset\n",
    "save_path = \"/scratch/users/hafezgh/fineweb-10B-tokenized\"\n",
    "processed_dataset.save_to_disk(save_path)                 # HF dataset format\n",
    "processed_dataset.to_parquet(f\"{save_path}.parquet\")      # Parquet format\n",
    "print(f\"✅ Processed dataset saved to {save_path} and {save_path}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (memory-2025-02-py310)",
   "language": "python",
   "name": "memory-2025-02-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
